{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will obtain all the earnings date from Dec. 2016 to Dec. 2020.\n",
    "\n",
    "NOTE: Not all of the stocks are reported by Yahoo Finance. Some stocks are reported by NASDAQ and Yahoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Goal:\n",
    "#### To extract all the earnings dates for every company in all quarters\n",
    "##### Method is similar to the Google_News_Web_Parsing.py method of web parsing but with Yahoo Finance\n",
    "###### Example URL: https://finance.yahoo.com/calendar/earnings?from=2020-03-01&to=2020-03-07&day=2020-03-05&offset=0&size=100\n",
    "\n",
    "import pandas, json, numpy, requests, os, datetime, pytz, tweepy, sqlite3, time, re, random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Need to gather the earnings date for at least 3 years to get a average date/range of dates of when earnings might occur\n",
    "## The \"Reported EPS\" and \"Estimated EPS\" and all other information will also be extracted (Earnings Per Share)\n",
    "\n",
    "# Creating two list of dates from 2016-2019: the first list = date of the beginning of the week, the second list = date of the end of that week\n",
    "# The range of the dates are from 2016-12-08 to 2020-01-04. Yahoo Finance does not provide many dates for 2016 and older.\n",
    "# So the first date for the first list is 2016-12-04 and the last date is 2020-12-27\n",
    "# For the second list, the first date is 2016-12-10 and the last date is 2021-01-2\n",
    "## A third list is needed for the days in-between those dates, which will be created as a temporary list in a loop\n",
    "begin_week_list = []\n",
    "end_week_list =  []\n",
    "week_counter = 0\n",
    "start_week = datetime.date(2016, 12, 4)\n",
    "x = 0\n",
    "\n",
    "while x != datetime.date(2021, 1, 3): # The first Sunday of 2021\n",
    "    begin_week_list.append(start_week + datetime.timedelta(days= week_counter * 7))\n",
    "    end_week_list.append(start_week + datetime.timedelta(days= (week_counter + 1) * 7 - 1))\n",
    "\n",
    "    week_counter += 1\n",
    "\n",
    "    x = start_week + datetime.timedelta(days= week_counter * 7)\n",
    "\n",
    "##### The length of both lists should be the same for the next part to work\n",
    "print(len(begin_week_list), len(end_week_list))\n",
    "\n",
    "#####____________________________________Defining a Function to Loop__________(See Bottom of Script for Explanation on Single URL Sample)____________\n",
    "## Creating a function to all the above to allow looping of all the dates:\n",
    "\n",
    "def yahoo_earnings_extraction(begin_week_list, end_week_list):\n",
    "    earnings_df_quarter_1 = pandas.DataFrame()\n",
    "    earnings_df_quarter_2 = pandas.DataFrame()\n",
    "    earnings_df_quarter_3 = pandas.DataFrame()\n",
    "    earnings_df_quarter_4 = pandas.DataFrame()      \n",
    "\n",
    "    for u in range(len(begin_week_list)):\n",
    "        begin_week_date = begin_week_list[u]\n",
    "        end_week_date = end_week_list[u]\n",
    "        weekdays_list = []\n",
    "        weekdays_counter = 1\n",
    "        offset_list = [0, 100, 200, 300] # This is controls the page numbers. There are up to 100 stocks listed in each page. 400 max earnings in a day is a guess.\n",
    "\n",
    "        for v in range(5):  # 0-4 for the 5 weekdays in the week; this creates a list of the weekdays starting from the beginning of the week\n",
    "            weekdays_list.append(begin_week_date + datetime.timedelta(days= weekdays_counter))\n",
    "            weekdays_counter += 1\n",
    "\n",
    "        for t in weekdays_list:\n",
    "            print(t)\n",
    "            for p in offset_list:\n",
    "                # Standard URL format used by Yahoo Finance\n",
    "                # from = start of the week\n",
    "                # to = end of the week\n",
    "                # day = specific weekday to examine\n",
    "                # offset = page number\n",
    "                # size = number of companies to dispay in one page; \n",
    "                ### If size is changed, offset needs to be changed to increment in that order: size = 50, then offset = [0, 50, 100, 150]\n",
    "                ### The defaul size is 100 and the max seems to be 100. Size parameter is not needed in URL and will default to 100\n",
    "                print(str(begin_week_date), str(end_week_date))\n",
    "                get_URL = \"https://finance.yahoo.com/calendar/earnings?from=\" + str(begin_week_date) + \"&to=\" + str(end_week_date) + \"&day=\" + str(t) + \"&offset=\" + str(p) + \"&size=100\" \n",
    "                Yahoo_Earnings_Date_requests = requests.get(get_URL)\n",
    "\n",
    "                soup = BeautifulSoup(Yahoo_Earnings_Date_requests.text, 'html.parser')\n",
    "\n",
    "                if bool(str(soup.find_all('td')) == '[]') == True: # This means that the offset value does not populate the webpage with earnings\n",
    "                    print(p, 'no offset')\n",
    "                    break\n",
    "\n",
    "                print(p, 'yes offset')\n",
    "\n",
    "                # Obtaining stock company name:\n",
    "                company_attribute_list = list(soup.find_all('td', attrs={'aria-label' :'Company'}))\n",
    "                company_name_list = []\n",
    "\n",
    "                for q in company_attribute_list:\n",
    "                    company_name = re.search('-->(.*)<!--', str(q))\n",
    "                    company_name_list.append(company_name.group(1))\n",
    "\n",
    "\n",
    "                # Obtianing symbols/tickers of stocks:\n",
    "                symbol_attribute_list = list(soup.find_all('td', attrs={'aria-label' :'Symbol'}))\n",
    "                symbol_name_list = []\n",
    "\n",
    "                for q in symbol_attribute_list:\n",
    "                    symbol_name = re.search('title=\"\">(.*)</a></td>', str(q))\n",
    "                    symbol_name_list.append(symbol_name.group(1))\n",
    "\n",
    "                # Obtaining \"Earnings Call Time\":\n",
    "                earnings_call_time_attribute_list = list(soup.find_all('td', attrs={'aria-label' :'Earnings Call Time'}))\n",
    "                earnings_call_time_list = []\n",
    "\n",
    "                for q in earnings_call_time_attribute_list:\n",
    "                    if bool(re.search('TAS', str(q))) == True:\n",
    "                        earnings_call_time = re.search('-->(.*)<!--', str(q))\n",
    "\n",
    "                    if bool(re.search('TAS', str(q))) == False:\n",
    "                        reactid = '\"' + str(int(re.search('data-reactid=\"(.*)\"><span data-reactid=', str(q)).group(1)) + 1) + '\"'\n",
    "                        earnings_call_time = re.search('\"><span data-reactid=' + reactid +'>(.*)</span>', str(q))\n",
    "                        \n",
    "                    earnings_call_time_list.append(earnings_call_time.group(1))\n",
    "\n",
    "\n",
    "                # Obtaining EPS estimate:\n",
    "                eps_estimate_attribute_list = list(soup.find_all('td', attrs={'aria-label' :'EPS Estimate'}))\n",
    "                eps_estimate_list = []\n",
    "\n",
    "                for q in eps_estimate_attribute_list:\n",
    "                    if bool(re.search('N/A', str(q))) == True: # <td aria-label=\"EPS Estimate\" class=\"Va(m) Ta(end) Pstart(15px) W(10%) Fz(s)\" colspan=\"\" data-reactid=\"315\"><span data-reactid=\"316\">N/A</span></td>\n",
    "                        eps_estimate = numpy.NaN\n",
    "                        eps_estimate_list.append(eps_estimate)\n",
    "\n",
    "                    if bool(re.search('N/A', str(q))) == False: # <td aria-label=\"EPS Estimate\" class=\"Va(m) Ta(end) Pstart(15px) W(10%) Fz(s)\" colspan=\"\" data-reactid=\"198\"><!-- react-text: 199 -->0.58<!-- /react-text --></td>\n",
    "                        reactid = str(int(re.search('data-reactid=\"(.*)\"><!-- react-text: ', str(q)).group(1)) + 1)\n",
    "                        eps_estimate = re.search('\"><!-- react-text: ' + reactid +' -->(.*)<!-- /react-text --></td>', str(q))\n",
    "                        \n",
    "                        eps_estimate_list.append(float(eps_estimate.group(1)))\n",
    "\n",
    "\n",
    "                # Obtaining EPS reported:\n",
    "                eps_reported_attribute_list = list(soup.find_all('td', attrs={'aria-label' :'Reported EPS'}))\n",
    "                eps_reported_list = []\n",
    "\n",
    "                for q in eps_reported_attribute_list:\n",
    "                    if bool(re.search('N/A', str(q))) == True: #  <td aria-label=\"Reported EPS\" class=\"Va(m) Ta(end) Pstart(15px) W(10%) Fz(s)\" colspan=\"\" data-reactid=\"57\"><span data-reactid=\"58\">N/A</span></td>\n",
    "                        eps_reported = numpy.NaN\n",
    "                        eps_reported_list.append(eps_reported)\n",
    "\n",
    "                    if bool(re.search('N/A', str(q))) == False: # <td aria-label=\"Reported EPS\" class=\"Va(m) Ta(end) Pstart(15px) W(10%) Fz(s)\" colspan=\"\" data-reactid=\"44\"><!-- react-text: 45 -->0.88<!-- /react-text --></td>\n",
    "                        reactid = str(int(re.search('data-reactid=\"(.*)\"><!-- react-text: ', str(q)).group(1)) + 1)\n",
    "                        eps_reported = re.search('\"><!-- react-text: ' + reactid +' -->(.*)<!-- /react-text --></td>', str(q))\n",
    "                        \n",
    "                        eps_reported_list.append(float(eps_reported.group(1)))\n",
    "\n",
    "\n",
    "                # Obtaining Surprise%:\n",
    "                surprise_attribute_list = list(soup.find_all('td', attrs={'aria-label' :'Surprise(%)'}))\n",
    "                surprise_list = []\n",
    " \n",
    "                for q in surprise_attribute_list:\n",
    "                    if bool(re.search('N/A', str(q))) == True: # <td aria-label=\"Surprise(%)\" class=\"Va(m) Ta(end) Px(15px) W(10%) Fz(s)\" colspan=\"\" data-reactid=\"59\"><span data-reactid=\"60\">N/A</span></td>\n",
    "                        surprise = numpy.NaN\n",
    "                        surprise_list.append(surprise)\n",
    "\n",
    "                    if bool(re.search('N/A', str(q))) == False: # <td aria-label=\"Surprise(%)\" class=\"Va(m) Ta(end) Px(15px) W(10%) Fz(s)\" colspan=\"\" data-reactid=\"46\"><span class=\"Trsdu(0.3s) Fw(600) C($dataRed)\" data-reactid=\"47\">-10.2</span></td>\n",
    "                        reactid = '\"' + str(int(re.search('data-reactid=\"(.*)\"><span class=', str(q)).group(1)) + 1) + '\"'\n",
    "                        surprise= re.search('\\)\" data-reactid=' + reactid +'>(.*)</span></td>', str(q))\n",
    "                        \n",
    "                        surprise_list.append(float(surprise.group(1)))\n",
    "\n",
    "                    \n",
    "                ### Creating a dictionary --> DataFrame:\n",
    "                temp_dict = {'Name' : company_name_list, 'Symbol' : symbol_name_list, \"Earnings_Call_Time\" : earnings_call_time_list, \"EPS_Estimate\" :  eps_estimate_list, \"EPS_Reported\" :  eps_reported_list, \"Surprise_Percentage\" : surprise_list}\n",
    "                temp_df = pandas.DataFrame(temp_dict)\n",
    "                temp_df['Date'] = str(t)\n",
    "\n",
    "                ### Removing duplicates:\n",
    "                for q in temp_df.Symbol:\n",
    "                    selected_df = temp_df[temp_df.Symbol == q]\n",
    "\n",
    "                    if len(selected_df) > 1:\n",
    "                        index_list = list(selected_df.index)\n",
    "                        new_index_list = []\n",
    "\n",
    "                        for s in index_list:   \n",
    "                            if selected_df.Earnings_Call_Time[s] == 'After Market Close':\n",
    "                                keeper = s\n",
    "\n",
    "                            elif selected_df.Earnings_Call_Time[s] == 'Time Not Supplied':\n",
    "                                keeper = s\n",
    "\n",
    "                            elif selected_df.Earnings_Call_Time[s] == 'TAS':\n",
    "                                keeper = s\n",
    "\n",
    "                            else:\n",
    "                                keeper = 'double'\n",
    "                                temp_df.drop_duplicates(subset= 'Symbol')\n",
    "\n",
    "                        if keeper == 'double':\n",
    "                            print(t, 'double')\n",
    "                            break\n",
    "\n",
    "                        for s in index_list:\n",
    "                            if s != keeper:\n",
    "                                new_index_list.append(s)\n",
    "\n",
    "                        print(new_index_list)\n",
    "                        temp_df = temp_df.drop(new_index_list)\n",
    "\n",
    "                temp_df = temp_df.reset_index(drop=True)\n",
    "                print(len(temp_df))\n",
    "            \n",
    "                for l in [2016, 2017, 2018, 2019, 2020]:\n",
    "                    if datetime.date(l, 1, 1) <= t <= datetime.date(l, 3, 31): # First Earnings Quarter\n",
    "                        earnings_df_quarter_1 = pandas.concat([earnings_df_quarter_1, temp_df])\n",
    "                        print('q1')\n",
    "\n",
    "                    elif datetime.date(l, 4, 1) <= t <= datetime.date(l, 6, 30): # Second Earnings Quarter\n",
    "                        earnings_df_quarter_2 = pandas.concat([earnings_df_quarter_2, temp_df])\n",
    "                        print('q2')\n",
    "                \n",
    "                    elif datetime.date(l, 7, 1) <= t <= datetime.date(l, 9, 30): # Third Earnings Quarter\n",
    "                        earnings_df_quarter_3 = pandas.concat([earnings_df_quarter_3, temp_df])\n",
    "                        print('q3')\n",
    "                \n",
    "                    elif datetime.date(l, 10, 1) <= t <= datetime.date(l, 12, 31): # Fourth Earnings Quarter\n",
    "                        earnings_df_quarter_4 = pandas.concat([earnings_df_quarter_4, temp_df])\n",
    "                        print('q4')\n",
    "\n",
    "            print('date end')\n",
    "            earnings_df_quarter_1\n",
    "            earnings_df_quarter_2\n",
    "            earnings_df_quarter_3\n",
    "            earnings_df_quarter_4\n",
    "\n",
    "    # Saving DataFrames\n",
    "    ### Note: THIS IS NOT SET TO UPDATE EXISTING FILES AND IS MEANT TO GATHER HISTORICAL DATA----OVERWRITE WILL OCCUR\n",
    "    if not os.path.exists(os.getcwd() + '\\\\Earnings Dates\\\\'):\n",
    "        os.makedirs(os.getcwd() + '\\\\Earnings Dates\\\\')\n",
    "\n",
    "    filepath = os.getcwd() + '\\\\Earnings Dates\\\\'\n",
    "\n",
    "    earnings_df_quarter_1.to_csv(filepath + 'Earnings - Quarter 1 - 2016-2020.csv')\n",
    "    earnings_df_quarter_2.to_csv(filepath + 'Earnings - Quarter 2 - 2016-2020.csv')\n",
    "    earnings_df_quarter_3.to_csv(filepath + 'Earnings - Quarter 3 - 2016-2020.csv')\n",
    "    earnings_df_quarter_4.to_csv(filepath + 'Earnings - Quarter 4 - 2016-2020.csv')\n",
    "\n",
    "\n",
    "yahoo_earnings_extraction(begin_week_list, end_week_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
