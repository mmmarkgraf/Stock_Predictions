{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, json, numpy, requests, os, datetime, pytz, tweepy, sqlite3, time, re, random, matplotlib.pyplot as plt, sklearn, statsmodels.api as sm\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is for giving commonly found words a value.\n",
    "\n",
    "The overall plan is to replace all words in a sentence with a value, which will be added to give a net value for the sentence. All other uncommon words will be ignored. Only headlines or snippets of New York Times articles with the company name in them will be considered due to many of the articles being irrelevant to the company. All google search results will be considered.\n",
    "\n",
    "NOTE: THIS SCRIPT REQUIRES MANUALLY ASSIGNING VALUES TO EACH WORD USING MICROSOFT EXCEL!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying the method to obtain symbols and company names: (Copied from Stats_Linear_Regression_Fit_Moving_Average_Pattern.py)\n",
    "NYSE_csv = pandas.read_csv('NYSE.txt', sep=\"\\t\", header=0).set_index('Symbol')\n",
    "\n",
    "AMEX_csv = pandas.read_csv('AMEX.txt', sep=\"\\t\", header=0).set_index('Symbol')\n",
    "\n",
    "stock_exchange_ticks_and_names = pandas.merge(NYSE_csv.reset_index(), AMEX_csv.reset_index(), how='outer')\n",
    "stock_exchange_ticks_and_names.to_csv('merged_NYSE_AMEX.csv')\n",
    "stock_exchange_ticks_and_names_copy = stock_exchange_ticks_and_names.copy().dropna()\n",
    "\n",
    "regex1 = re.compile('[@_!#$%^&*()<>?/\\|}{~:[\\].]')\n",
    "regex2 = re.compile('Cl ')\n",
    "\n",
    "stock_exchange_ticks_and_names_removed = pandas.DataFrame()\n",
    "\n",
    "for x, y in stock_exchange_ticks_and_names_copy.iterrows():\n",
    "\n",
    "    if bool(regex1.search(y['Description'])) == False and bool(regex2.search(y['Description'])) == False and bool(regex1.search(y['Symbol'])) == False:\n",
    "        stock_exchange_ticks_and_names_removed.loc[x, 'Symbol'] = y['Symbol']\n",
    "        stock_exchange_ticks_and_names_removed.loc[x, 'Description'] = y['Description']\n",
    "\n",
    "stock_indices_df = pandas.DataFrame({'Description': ['S&P', 'Dow', \"Nasdaq\"], 'Symbol': ['.INX', '.DJI', \".IXIC\"]})\n",
    "\n",
    "stocks_and_names_with_indices = pandas.concat([stock_exchange_ticks_and_names_removed, stock_indices_df])\n",
    "stocks_and_names_with_indices = stocks_and_names_with_indices.set_index('Symbol')\n",
    "stocks_and_names_with_indices = stocks_and_names_with_indices.reset_index()\n",
    "stocks_and_names_with_indices.to_csv('merged_NYSE_AMEX_word_extraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extraction(name):\n",
    "    ### Loading in NY Times and Google data:\n",
    "    times_df = pandas.read_csv(os.getcwd() + '\\\\Historical Articles\\\\' + name + '.csv')\n",
    "    google_df = pandas.read_csv(os.getcwd() + '\\\\Google Search - News\\\\' + name + '.csv')\n",
    "    \n",
    "    ### Finding the most common words in NY Times:\n",
    "    times_all_words_list = []\n",
    "    times_articles_count = 0\n",
    "\n",
    "    for x, y in times_df.iterrows():\n",
    "        u = None\n",
    "        if name in str(y['snippet']):\n",
    "            u = 'snippet'\n",
    "        elif name in str(y['abstract']):\n",
    "            u = 'abstract'\n",
    "        elif name in str(y['headline']):\n",
    "            u = 'headline'\n",
    "\n",
    "        if u is None:\n",
    "            continue\n",
    "\n",
    "        elif type(u) == str:\n",
    "            string_extraction = y[u]\n",
    "\n",
    "        u = None\n",
    "\n",
    "        try:\n",
    "            string_extraction_replaced = string_extraction.replace(name, '')\n",
    "        \n",
    "            string_extraction_replaced = string_extraction_replaced('(' + symbol + ')', '')\n",
    "        \n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        string_extraction_split = string_extraction.split()\n",
    "\n",
    "        string_extraction_unique = list(set(string_extraction_split))\n",
    "\n",
    "        times_all_words_list.extend(string_extraction_unique)\n",
    "\n",
    "        times_articles_count += 1\n",
    "\n",
    "    print('Number of Times Articles Extracted: ', times_articles_count)\n",
    "    \n",
    "    ### Finding the most common words in Google News Web Search Results:\n",
    "    google_all_words_list = []\n",
    "    google_articles_count = 0\n",
    "\n",
    "    for x, y in google_df.iterrows():\n",
    "        string_extraction = y['headline'] + ' ' + y['snippet']\n",
    "\n",
    "        try:\n",
    "            string_extraction_replaced = string_extraction.replace(name, '')\n",
    "        \n",
    "            string_extraction_replaced = string_extraction_replaced('(' + symbol + ')', '')\n",
    "        \n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        string_extraction_split = string_extraction_replaced.split()\n",
    "\n",
    "        string_extraction_unique = list(set(string_extraction_split))\n",
    "\n",
    "        google_all_words_list.extend(string_extraction_unique)\n",
    "\n",
    "        google_articles_count += 1\n",
    "\n",
    "    print('Number of Google Search Results Extracted: ', google_articles_count)\n",
    "    \n",
    "    return times_all_words_list, google_all_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_counting(times_all_words_list, google_all_words_list):\n",
    "    # Counting the times the words appeared uniquely:\n",
    "    combined_words_list = []\n",
    "    combined_words_list.extend(google_all_words_list)\n",
    "    combined_words_list.extend(times_all_words_list)\n",
    "    combined_unique_list = list(set(combined_words_list))\n",
    "    combined_df = pandas.DataFrame()\n",
    "\n",
    "    for x, y in enumerate(combined_unique_list):    \n",
    "        combined_df.loc[x, 'Word'] = y\n",
    "        combined_df.loc[x, 'Count'] = combined_words_list.count(y)\n",
    "        \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through all stocks:\n",
    "for u, v in stocks_and_names_with_indices.iterrows():\n",
    "    try:\n",
    "        times_all_words_list, google_all_words_list = word_extraction(v['Description'])\n",
    "    except Exception:\n",
    "        continue\n",
    "    combined_df = words_counting(times_all_words_list, google_all_words_list)\n",
    "    temp_df = pandas.DataFrame()\n",
    "\n",
    "    if u == 0:\n",
    "        total_df = combined_df.copy()\n",
    "    \n",
    "    else:\n",
    "        for i, t in combined_df.iterrows():\n",
    "            if bool(type(total_df.Count[total_df.Word == t.Word]) == pandas.Series) == True:\n",
    "                total_df.Count[total_df.Word == t.Word] = total_df.Count[total_df.Word == t.Word] + t.Count\n",
    "            else:\n",
    "                total_df = pandas.concat([total_df, {'Word': t.Word, 'Count': t.Count}])\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the total word counts in all Times and Google results:\n",
    "total_df.to_csv('All Word Extraction Counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphing the counts:\n",
    "plt.hist(total_df.Count, bins= 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____Manually Assinging Values______\n",
    "\n",
    "For this part, I will be assigning two values to each word: a Sentence_Value and a Importance_Value.\n",
    "\n",
    "Sentence_Value = the value the word will contribute to the net value of the sentence (sell, buy, drop, increase, etc.)\n",
    "Sentence values are additions, in the range of -10 to +10\n",
    "\n",
    "Importance_Value = the value the word means for the stock in general (earnings, bankruptcy, etc.)\n",
    "Importance values are multipliers, in the range of 0 to +10\n",
    "\n",
    "Example:\n",
    "Sentence = the stock will increase in price due to earnings\n",
    "Conversion = 0 0 0 +5 0 (+ 0, *2) 0 0 (+0, *10)\n",
    "Sentence Net Value = 5 * (10 + 2) =  60\n",
    "Here, the word increase is has a sentence value of +5, while price and earnings have 0 sentence values but instead, add to the multiplier of the overall net value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing all text with 0 values for Sentence_Value and Importance_Value.\n",
    "total_df_valuated = pandas.read_csv('All Word Extraction Counts - Valued.csv')\n",
    "\n",
    "no_value_index_list = list(total_df_valuated.index[(total_df_valuated.Sentence_Value == 0) & (total_df_valuated.Importance_Value == 0)])\n",
    "\n",
    "total_df_valuated_removed = total_df_valuated.drop(no_value_index_list).sort_values('Word').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_valuated_removed.to_csv('Only Valued Words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a DataFrame with all the text that contain the company name, then using the manual valuation as the key to \n",
    "### to assigning values. This is for checking to see how the manual valuation performs. The first part of the definition is \n",
    "### copied from the word_extraction definition.\n",
    "def text_valuate(symbol, name):\n",
    "    ### Loading in NY Times and Google data:\n",
    "    times_df = pandas.read_csv(os.getcwd() + '\\\\Historical Articles\\\\' + name + '.csv')\n",
    "    google_df = pandas.read_csv(os.getcwd() + '\\\\Google Search - News\\\\' + name + '.csv')\n",
    "    \n",
    "    print(symbol)\n",
    "    \n",
    "    text_list = []\n",
    "    text_valuation_list = []\n",
    "    sentence_value_list = []\n",
    "    importance_value_list = []\n",
    "    \n",
    "    ### Finding the most common words in NY Times:\n",
    "    times_articles_count = 0\n",
    "    \n",
    "    for x, y in times_df.iterrows():\n",
    "        u = None\n",
    "        if name in str(y['snippet']):\n",
    "            u = 'snippet'\n",
    "        elif name in str(y['abstract']):\n",
    "            u = 'abstract'\n",
    "        elif name in str(y['headline']):\n",
    "            u = 'headline'\n",
    "\n",
    "        if u is None:\n",
    "            continue\n",
    "\n",
    "        elif type(u) == str:\n",
    "            string_extraction = y[u]\n",
    "\n",
    "        u = None\n",
    "\n",
    "        string_extraction_split = string_extraction.split()\n",
    "\n",
    "        sentence_value = 0\n",
    "        importance_value = 0\n",
    "        \n",
    "        for u in range(len(string_extraction_split)):\n",
    "            for c, v in total_df_valuated_removed.iterrows():\n",
    "                if bool(string_extraction_split[u] == v.Word) == True:\n",
    "                    sentence_value = sentence_value + v.Sentence_Value\n",
    "                    importance_value = importance_value + v.Importance_Value\n",
    "\n",
    "        text_list.append(string_extraction)      \n",
    "        net_sentence_value = sentence_value * importance_value\n",
    "        text_valuation_list.append(net_sentence_value)\n",
    "        sentence_value_list.append(sentence_value)\n",
    "        importance_value_list.append(importance_value)\n",
    "        times_articles_count += 1\n",
    "\n",
    "    print('Number of Times Articles Extracted: ', times_articles_count)\n",
    "    print('Length of Times Articles Extracted: ', len(text_list), \"; Times Valuation List: \", len(text_valuation_list))\n",
    "    \n",
    "    google_articles_count = 0\n",
    "    for x, y in google_df.iterrows():\n",
    "        string_extraction = y['headline'] + ' ' + y['snippet']\n",
    "\n",
    "        string_extraction_split = string_extraction.split()\n",
    "        \n",
    "        string_extraction_unique = list(set(string_extraction_split))  ## This remains because the headline and snippet might be the same\n",
    "\n",
    "        sentence_value = 0\n",
    "        importance_value = 0\n",
    "        \n",
    "        for u in range(len(string_extraction_unique)):\n",
    "            for c, v in total_df_valuated_removed.iterrows():\n",
    "                if bool(string_extraction_split[u] == v.Word) == True:\n",
    "                    sentence_value = sentence_value + v.Sentence_Value\n",
    "                    importance_value = importance_value + v.Importance_Value\n",
    "\n",
    "        text_list.append(string_extraction)                    \n",
    "        net_sentence_value = sentence_value * importance_value\n",
    "        text_valuation_list.append(net_sentence_value)\n",
    "        sentence_value_list.append(sentence_value)\n",
    "        importance_value_list.append(importance_value)\n",
    "        google_articles_count += 1                    \n",
    "                \n",
    "    print('Number of Google Search Results Extracted: ', google_articles_count)\n",
    "    print('Length of Times & Google Articles Extracted: ', len(text_list), \"; Times & Google Valuation List: \", len(text_valuation_list))\n",
    "\n",
    "    return text_list, sentence_value_list, importance_value_list, text_valuation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through all stocks:\n",
    "for u, v in stocks_and_names_with_indices.iterrows():\n",
    "    try:\n",
    "        text_list, sentence_value_list, importance_value_list, text_valuation_list = text_valuate(v['Symbol'], v['Description'])\n",
    "\n",
    "        text_dict = {'Text': text_list, 'Total_Sentence_Value': sentence_value_list, 'Total_Importance_Value': importance_value_list, 'Net_Sentence_Value': text_valuation_list}\n",
    "        temp_df = pandas.DataFrame(text_dict)\n",
    "\n",
    "        if u == 0:\n",
    "            text_df = temp_df.copy()\n",
    "\n",
    "        else:\n",
    "            text_df = pandas.concat([text_df, temp_df])\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('Valued Articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually checking to see if any the word valuation gave importance to relevant text: \n",
    "### Most of the text are valued good enough as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modifying text_valuate to add the Sentence_Value, Importance_Value, and Text_value (Net_Sentence_Value) to the .csv\n",
    "### file that contains the text information:\n",
    "def text_valuate_csv_modify(symbol, name):\n",
    "    ### Loading in NY Times and Google data:\n",
    "    times_df = pandas.read_csv(os.getcwd() + '\\\\Historical Articles\\\\' + name + '.csv')\n",
    "    google_df = pandas.read_csv(os.getcwd() + '\\\\Google Search - News\\\\' + name + '.csv')\n",
    "    total_df_valuated_removed = pandas.read_csv('Only Valued Words.csv')\n",
    "    \n",
    "    print(symbol)\n",
    "    \n",
    "    ### Finding the most common words in NY Times:\n",
    "    times_articles_count = 0\n",
    "    \n",
    "    for x, y in times_df.iterrows():\n",
    "        sentence_value = 0\n",
    "        importance_value = 0\n",
    "        text_value = 0\n",
    "\n",
    "        try:\n",
    "            if 'Text_Value' in times_df.columns:  ## This is to speed up the script if the text has been evaluated before\n",
    "                if times_df.loc[x, 'Sentence_Value'] > 0:\n",
    "                    continue\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        u = None\n",
    "\n",
    "        if name in str(y['snippet']):\n",
    "            u = 'snippet'\n",
    "        elif name in str(y['abstract']):\n",
    "            u = 'abstract'\n",
    "        elif name in str(y['headline']):\n",
    "            u = 'headline'\n",
    "\n",
    "        if u is None:\n",
    "            times_df.loc[x, 'Sentence_Value'] = 0\n",
    "            times_df.loc[x, 'Importance_Value'] = 0\n",
    "            times_df.loc[x, 'Text_Value'] = 0            \n",
    "            continue\n",
    "\n",
    "        string_extraction_split = y[u].split()\n",
    "\n",
    "        for u in range(len(string_extraction_split)):\n",
    "            for c, v in total_df_valuated_removed.iterrows():\n",
    "                if bool(string_extraction_split[u] == v.Word) == True:\n",
    "                    sentence_value = sentence_value + v.Sentence_Value\n",
    "                    importance_value = importance_value + v.Importance_Value\n",
    "\n",
    "        times_df.loc[x, 'Sentence_Value'] = sentence_value\n",
    "        times_df.loc[x, 'Importance_Value'] = importance_value\n",
    "        times_df.loc[x, 'Text_Value'] = sentence_value * importance_value\n",
    "\n",
    "        times_articles_count += 1\n",
    "\n",
    "    print('Number of Times Articles Extracted: ', times_articles_count)\n",
    "\n",
    "    google_articles_count = 0\n",
    "    for x, y in google_df.iterrows():\n",
    "\n",
    "        if 'Text_Value' in google_df.columns:  ## This is to speed up the script if the text has been evaluated before; Remove if word values df is changed\n",
    "            if google_df.loc[x, 'Sentence_Value'] > 0:\n",
    "                continue\n",
    "\n",
    "        string_extraction = y['headline'] + ' ' + y['snippet']\n",
    "\n",
    "        string_extraction_split = string_extraction.split()\n",
    "\n",
    "        string_extraction_unique = list(set(string_extraction_split))  ## This remains because the headline and snippet might be the same\n",
    "\n",
    "        sentence_value = 0\n",
    "        importance_value = 0\n",
    "        for u in range(len(string_extraction_unique)):\n",
    "            for c, v in total_df_valuated_removed.iterrows():\n",
    "                if bool(string_extraction_split[u] == v.Word) == True:\n",
    "                    sentence_value = sentence_value + v.Sentence_Value\n",
    "                    importance_value = importance_value + v.Importance_Value\n",
    "\n",
    "        google_df.loc[x, 'Sentence_Value'] = sentence_value\n",
    "        google_df.loc[x, 'Importance_Value'] = importance_value\n",
    "        google_df.loc[x, 'Text_Value'] = sentence_value * importance_value\n",
    "\n",
    "        google_articles_count += 1                    \n",
    "\n",
    "    print('Number of Google Search Results Extracted: ', google_articles_count)\n",
    "    \n",
    "    times_df.to_csv(os.getcwd() + '\\\\Historical Articles\\\\' + name + '.csv')\n",
    "    google_df.to_csv(os.getcwd() + '\\\\Google Search - News\\\\' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through all stocks:\n",
    "for u, v in stocks_and_names_with_indices.iloc[2000:].iterrows():\n",
    "    try:\n",
    "        text_valuate_csv_modify(v['Symbol'], v['Description'])\n",
    "\n",
    "    except Exception:\n",
    "        print('Error: ', v['Description'])\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
