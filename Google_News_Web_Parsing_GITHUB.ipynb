{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################### ONLY RUN 50 STOCKS AT A TIME OR LOWER THE PAGES TO 10><br><br>\n",
    "This script is to perform web scraping on Google search (news tab). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to parse searches Google.com (news) for the stock symbols/company names.\n",
    "# Default web searching URL is as follows:\n",
    "    # https://www.google.com/search?q=COMPANY_NAME&tbm=nws&start=PAGE_NUMBER\n",
    "        # Replace \"COMPANY_NAME\" with search term and \"PAGE_NUMER\" with 0 = page 1, 10 = page 2, 20 = page 3, etc.\n",
    "# URL/REST parameters/search options: https://developers.google.com/custom-search/v1/cse/list \n",
    "## Google API for Python is available: https://github.com/googleapis/google-api-python-client \n",
    "\n",
    "import pandas, json, numpy, requests, os, datetime, pytz, tweepy, sqlite3, time, re, random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# BeautifulSoup Doc: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "#### Following the same first steps from the Times API script:\n",
    "import pandas, json, numpy, requests, os, datetime, pytz, tweepy, sqlite3, time, re\n",
    "\n",
    "NYSE_csv = pandas.read_csv('NYSE.txt', sep=\"\\t\", header=0).set_index('Symbol')\n",
    "NYSE_csv.to_csv('NYSE_csv.csv')\n",
    "\n",
    "AMEX_csv = pandas.read_csv('AMEX.txt', sep=\"\\t\", header=0).set_index('Symbol')\n",
    "AMEX_csv.to_csv('AMEX_csv.csv')\n",
    "\n",
    "stock_exchange_ticks_and_names = pandas.merge(NYSE_csv.reset_index(), AMEX_csv.reset_index(), how='outer')\n",
    "stock_exchange_ticks_and_names.to_csv('merged_NYSE_AMEX.csv')\n",
    "stock_exchange_ticks_and_names_copy = stock_exchange_ticks_and_names.copy().dropna()\n",
    "\n",
    "regex1 = re.compile('[@_!#$%^&*()<>?/\\|}{~:[\\].]')\n",
    "regex2 = re.compile('Cl ')\n",
    "\n",
    "stock_exchange_ticks_and_names_removed = pandas.DataFrame()\n",
    "\n",
    "for x, y in stock_exchange_ticks_and_names_copy.iterrows():\n",
    "\n",
    "    if bool(regex1.search(y['Description'])) == False and bool(regex2.search(y['Description'])) == False and bool(regex1.search(y['Symbol'])) == False:\n",
    "        stock_exchange_ticks_and_names_removed.loc[x, 'Symbol'] = y['Symbol']\n",
    "        stock_exchange_ticks_and_names_removed.loc[x, 'Description'] = y['Description']\n",
    "\n",
    "stock_exchange_ticks_and_names_removed = stock_exchange_ticks_and_names_removed.set_index('Symbol')\n",
    "stock_exchange_ticks_and_names_removed = stock_exchange_ticks_and_names_removed.reset_index()\n",
    "stock_exchange_ticks_and_names_removed.to_csv('merged_NYSE_AMEX_removed_google_news.csv')\n",
    "\n",
    "### To account for the Times API daily limit and time of running the script, each list is limited to 200: \n",
    "stock_exchange_ticks_and_names_0_399 = stock_exchange_ticks_and_names_removed.iloc[0:400, :]\n",
    "stock_exchange_ticks_and_names_400_799 = stock_exchange_ticks_and_names_removed.iloc[400:800, :]\n",
    "stock_exchange_ticks_and_names_800_1199 = stock_exchange_ticks_and_names_removed.iloc[800:1200, :]\n",
    "stock_exchange_ticks_and_names_1200_1599 = stock_exchange_ticks_and_names_removed.iloc[1200:1600, :]\n",
    "stock_exchange_ticks_and_names_1600_1999 = stock_exchange_ticks_and_names_removed.iloc[1600:2000, :]\n",
    "stock_exchange_ticks_and_names_2000_2399 = stock_exchange_ticks_and_names_removed.iloc[2000:2400, :]\n",
    "stock_exchange_ticks_and_names_2400_2799 = stock_exchange_ticks_and_names_removed.iloc[2400:2800, :]\n",
    "stock_exchange_ticks_and_names_2800_3199 = stock_exchange_ticks_and_names_removed.iloc[2800:3200, :]\n",
    "stock_exchange_ticks_and_names_3200_3599 = stock_exchange_ticks_and_names_removed.iloc[3200:3600, :]\n",
    "stock_exchange_ticks_and_names_3600_3999 = stock_exchange_ticks_and_names_removed.iloc[3600:4000, :]\n",
    "stock_exchange_ticks_and_names_4000_4399 = stock_exchange_ticks_and_names_removed.iloc[4000:4400, :]\n",
    "stock_exchange_ticks_and_names_4400_4799 = stock_exchange_ticks_and_names_removed.iloc[4400:4800, :]\n",
    "stock_exchange_ticks_and_names_4800_4880 = stock_exchange_ticks_and_names_removed.iloc[4800:, :]\n",
    "\n",
    "stock_indices_df = pandas.DataFrame({'Description': ['S%26P', 'Dow', \"Nasdaq\"], 'Symbol': ['.INX', '.DJI', \".IXIC\"]})\n",
    "\n",
    "\n",
    "## Google Search Parsing Starts: _____________________________________________________________________________\n",
    "\n",
    "def google_search_news(search_term, pages=10):\n",
    "    pages = list(numpy.arange(0, pages*10, 10))\n",
    "    filepath = os.getcwd() + '\\\\Google Search - News\\\\'\n",
    "\n",
    "    for x in search_term.Description:\n",
    "        print(x) ### This is to indicate where a error might have occurred.\n",
    "\n",
    "        dictionary_all = {}\n",
    "        master_df = pandas.DataFrame()\n",
    "        headline_list = []\n",
    "        web_url_list = []\n",
    "        source_list = []\n",
    "        time_list = []\n",
    "        snippet_list = []\n",
    "\n",
    "        for y in pages:\n",
    "            get_URL = \"https://www.google.com/search?q=\" + x + \"&tbm=nws&start=\" + str(y) \n",
    "            Google_requests = requests.get(get_URL)\n",
    "\n",
    "            soup = BeautifulSoup(Google_requests.text, 'html.parser')\n",
    "            \n",
    "            # Headline Parsing: ----------------------------------------------------------------------------\n",
    "            temp_list = list(soup.find_all(\"div\", class_ =\"BNeawe vvjwJb AP7Wnd\"))\n",
    "            \n",
    "\n",
    "            for u in temp_list:\n",
    "                string = str(u)\n",
    "                result = re.search('<div class=\"BNeawe vvjwJb AP7Wnd\">(.*)</div>', string)\n",
    "                headline_list.append(result.group(1))\n",
    "\n",
    "            # Source Parsing: -------------------------------------------------------------------------------\n",
    "            temp_list = list(soup.find_all(\"div\", class_ =\"BNeawe UPmit AP7Wnd\"))\n",
    "\n",
    "            for u in temp_list:\n",
    "                string = str(u)\n",
    "                result = re.search('<div class=\"BNeawe UPmit AP7Wnd\">(.*)</div>', string)\n",
    "                if result is None:\n",
    "                    no_source = ''\n",
    "                    source_list.append(no_source)\n",
    "                else:\n",
    "                    source_list.append(result.group(1))\n",
    "                \n",
    "            # Snippet: ------------------------------------------------------------------------------------            \n",
    "            temp_list = list(soup.find_all(\"div\", class_ =\"BNeawe s3v9rd AP7Wnd\"))\n",
    "\n",
    "            for u in temp_list:\n",
    "                string = str(u)\n",
    "                if bool(re.search('<span class=\"r0bn4c rQMQod\"> · </span>(.*)</div></div></div>', string)) == True:\n",
    "                    result = re.search('<span class=\"r0bn4c rQMQod\"> · </span>(.*)</div></div></div>', string)\n",
    "                    snippet_list.append(result.group(1))\n",
    "\n",
    "            # Date Posted (approximate): ------------------------------------------------------------------\n",
    "            temp_list = list(soup.find_all(\"span\", class_ =\"r0bn4c rQMQod\"))\n",
    "            current_time = datetime.datetime.now()\n",
    "\n",
    "            for u in temp_list:\n",
    "                string = str(u)\n",
    "                if bool(re.search('<span class=\"r0bn4c rQMQod\">(.*)ago</span>', string)) == True:\n",
    "                    result = re.search('<span class=\"r0bn4c rQMQod\">(.*)ago</span>', string)\n",
    "                    \n",
    "                    digits = int(re.search('\\d*', result.group(1)).group(0))\n",
    "                    \n",
    "                    if 'min' in result.group(1):\n",
    "                        diff_time = datetime.timedelta(digits)\n",
    "                        \n",
    "                    elif 'hour' in result.group(1):\n",
    "                        diff_time = datetime.timedelta(digits)\n",
    "                    \n",
    "                    elif 'day' in result.group(1):\n",
    "                        diff_time = datetime.timedelta(digits)\n",
    "                        \n",
    "                    elif 'week' in result.group(1):\n",
    "                        diff_time = datetime.timedelta(digits)\n",
    "                        \n",
    "                    elif 'month' in result.group(1):\n",
    "                        diff_time = datetime.timedelta(digits)\n",
    "                        \n",
    "                    elif 'year' in result.group(1):\n",
    "                        diff_time = datetime.timedelta(digits)\n",
    "                    \n",
    "                    past_time = current_time - diff_time\n",
    "                    \n",
    "                    time_list.append(past_time.isoformat())\n",
    "\n",
    "        # Merging all lists into a dictionary. Then the dictionary into a DataFrame:\n",
    "        print(len(headline_list))\n",
    "        print(len(snippet_list))\n",
    "        print(len(time_list))\n",
    "        print(len(source_list))\n",
    "        dictionary_all = {'headline': headline_list, 'snippet': snippet_list, 'pub_date': time_list, 'source': source_list}\n",
    "        combined_df = pandas.DataFrame(dictionary_all)\n",
    "        combined_df['Company'] = x\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        # Same file saving code as in Times API script (except duplicate matching is done on headline instead of date):\n",
    "        # Matching is done on headline because it will eliminate \"reblogged\" articles.\n",
    "        if os.path.exists(filepath + x + '.csv') == True:\n",
    "            temp_df = pandas.read_csv(filepath + x + '.csv')\n",
    "            print('2')\n",
    "            #### This should cover the situation where pages != 1\n",
    "            master_df = pandas.concat([temp_df, combined_df], ignore_index=True).drop_duplicates('headline').sort_values(by='pub_date', ascending=False)\n",
    "            print('3')\n",
    "            master_df.to_csv(filepath + x + '.csv')\n",
    "        print('7')\n",
    "        if os.path.exists(filepath + x + '.csv') == False:\n",
    "            print('6')\n",
    "            combined_df.to_csv(filepath + x + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_news(stock_indices_df) ###\n",
    "\n",
    "google_search_news(stock_exchange_ticks_and_names_0_399)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_400_799)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_800_1199)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_1200_1599)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_1600_1999)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_2000_2399)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_2400_2799)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_2800_3199)  ###\n",
    "google_search_news(stock_exchange_ticks_and_names_3200_3599)  ###\n",
    "google_search_news(stock_exchange_ticks_and_names_3600_3999)  ###\n",
    "google_search_news(stock_exchange_ticks_and_names_4000_4399)  ### \n",
    "google_search_news(stock_exchange_ticks_and_names_4400_4799)  ###\n",
    "google_search_news(stock_exchange_ticks_and_names_4800_4880)  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
