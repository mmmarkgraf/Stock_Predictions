{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################# THIS SCRIPT REQUIRES AN API KEY FROM NEW YORK TIMES<br><br>\n",
    "\n",
    "This API has daily limitations for free users and so the defintions are required to run one per day.<br><br>\n",
    "\n",
    "Do note that the articles return will most likely not be relevant to the keyword used to perform the query search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, json, numpy, requests, os, datetime, pytz, tweepy, sqlite3, time, re\n",
    "\n",
    "# 3. Extracting news data from New York Times (NYT) (https://developer.nytimes.com/)\n",
    "## Genernal tutorial: (https://medium.com/@danalindquist/using-new-york-times-api-and-jq-to-collect-news-data-a5f386c7237b)\n",
    "\n",
    "### Goal for this script is to gather all histroical data about the company and align the article to the price of that day.\n",
    "### Restraints are to whether or not NYT reported it or not, or the history passes beyond 1985.\n",
    "### However, since the Alpha Vantage API does not provide daily price data beyond 20 years, the 1985 restraint is ignored.\n",
    "\n",
    "\n",
    "#### Repeating first few steps from Alpha Vantage API to generate a merged DataFrame of stock names and ticks.\n",
    "#### Change is lowering the amount of stocks/ticks per variable:\n",
    "NYSE_csv = pandas.read_csv('NYSE.txt', sep=\"\\t\", header=0).set_index('Symbol')\n",
    "NYSE_csv.to_csv('NYSE_csv.csv')\n",
    "\n",
    "AMEX_csv = pandas.read_csv('AMEX.txt', sep=\"\\t\", header=0).set_index('Symbol')\n",
    "AMEX_csv.to_csv('AMEX_csv.csv')\n",
    "\n",
    "stock_exchange_ticks_and_names = pandas.merge(NYSE_csv.reset_index(), AMEX_csv.reset_index(), how='outer')\n",
    "stock_exchange_ticks_and_names.to_csv('merged_NYSE_AMEX.csv')\n",
    "stock_exchange_ticks_and_names_copy = stock_exchange_ticks_and_names.copy().dropna()\n",
    "\n",
    "regex1 = re.compile('[@_!#$%^&*()<>?/\\|}{~:[\\].]')\n",
    "regex2 = re.compile('Cl ')\n",
    "\n",
    "stock_exchange_ticks_and_names_removed = pandas.DataFrame()\n",
    "\n",
    "for x, y in stock_exchange_ticks_and_names_copy.iterrows():\n",
    "\n",
    "    if bool(regex1.search(y['Description'])) == False and bool(regex2.search(y['Description'])) == False and bool(regex1.search(y['Symbol'])) == False:\n",
    "        stock_exchange_ticks_and_names_removed.loc[x, 'Symbol'] = y['Symbol']\n",
    "        stock_exchange_ticks_and_names_removed.loc[x, 'Description'] = y['Description']\n",
    "\n",
    "stock_exchange_ticks_and_names_removed = stock_exchange_ticks_and_names_removed.set_index('Symbol')\n",
    "stock_exchange_ticks_and_names_removed = stock_exchange_ticks_and_names_removed.reset_index()\n",
    "stock_exchange_ticks_and_names_removed.to_csv('merged_NYSE_AMEX_removed_times.csv')\n",
    "\n",
    "### To account for the Times API daily limit and time of running the script, each list is limited to 200: \n",
    "stock_exchange_ticks_and_names_0_399 = stock_exchange_ticks_and_names_removed.iloc[0:400, :]\n",
    "stock_exchange_ticks_and_names_400_799 = stock_exchange_ticks_and_names_removed.iloc[400:800, :]\n",
    "stock_exchange_ticks_and_names_800_1199 = stock_exchange_ticks_and_names_removed.iloc[800:1200, :]\n",
    "stock_exchange_ticks_and_names_1200_1599 = stock_exchange_ticks_and_names_removed.iloc[1200:1600, :]\n",
    "stock_exchange_ticks_and_names_1600_1999 = stock_exchange_ticks_and_names_removed.iloc[1600:2000, :]\n",
    "stock_exchange_ticks_and_names_2000_2399 = stock_exchange_ticks_and_names_removed.iloc[2000:2400, :]\n",
    "stock_exchange_ticks_and_names_2400_2799 = stock_exchange_ticks_and_names_removed.iloc[2400:2800, :]\n",
    "stock_exchange_ticks_and_names_2800_3199 = stock_exchange_ticks_and_names_removed.iloc[2800:3200, :]\n",
    "stock_exchange_ticks_and_names_3200_3599 = stock_exchange_ticks_and_names_removed.iloc[3200:3600, :]\n",
    "stock_exchange_ticks_and_names_3600_3999 = stock_exchange_ticks_and_names_removed.iloc[3600:4000, :]\n",
    "stock_exchange_ticks_and_names_4000_4399 = stock_exchange_ticks_and_names_removed.iloc[4000:4400, :]\n",
    "stock_exchange_ticks_and_names_4400_4799 = stock_exchange_ticks_and_names_removed.iloc[4400:4800, :]\n",
    "stock_exchange_ticks_and_names_4800_4880 = stock_exchange_ticks_and_names_removed.iloc[4800:, :]\n",
    "\n",
    "\n",
    "### Creating a list of terms to search for that is not a company name: (https://www.investing.com/indices/major-indices)\n",
    "### S&P = S%26P since & is a reserved character in URL\n",
    "stock_indices_df = pandas.DataFrame({'Description': ['S%26P', 'Dow', \"Nasdaq\"], 'Symbol': ['.INX', '.DJI', \".IXIC\"]})\n",
    "\n",
    "#### Used the DataFrame.Symbol and .Description as the keywords to look for article searching:\n",
    "\n",
    "## The \"pagination_list\" variable is required since there is a 10 request per minute restriction. See https://developer.nytimes.com/docs/articlesearch-product/1/overview\n",
    "## in the \"Pagination\" section for more info. The number of pages that I will be setting is 2 to save time. In theory,\n",
    "## the range should be up to 100 pages (even though there might not be 1000 articles on the company/keyword).\n",
    "\n",
    "def article_search(stock_list, pages= 2, begin_date= '20000101'):\n",
    "    pagination_list = list(range(pages))  \n",
    "    master_temp_df = pandas.DataFrame()\n",
    "    filepath = os.getcwd() + '\\\\Historical Articles\\\\'\n",
    "\n",
    "    for y in stock_list.Description:\n",
    "        print(y) ### This is to indicate where a error might have occurred.\n",
    "\n",
    "        y_splited = y.split()  ### This is done to remove the company type from the name and add a + for spaces.\n",
    "        if len(y_splited) == 1:  ### Want to only pass the first/only or first two words in the company name in URL.\n",
    "            y_modified = y_splited\n",
    "            print(y_modified)\n",
    "\n",
    "        if len(y_splited) > 1 and y_splited[1] != 'LP' and y_splited[1] != 'Llc' and y_splited[1] != 'Ltd' and y_splited[1] != 'Corp' and y_splited[1] != 'Inc' and y_splited[1] != 'Company' and y_splited[1] != 'ETF':\n",
    "            y_modified = y_splited[0] + '+' + y_splited[1]\n",
    "            print(y_modified)\n",
    "\n",
    "        else:\n",
    "            y_modified = y_splited[0]\n",
    "            print(y_modified)\n",
    "\n",
    "        for x in pagination_list:\n",
    "            get_URL = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=body:('\" + y_modified + \"')ANDnews_desk:('Business')&begin_date=\" + begin_date + \"&sort=newest&page=\" + str(x) + \"&api-key=\" ### INSERT API KEY\n",
    "            time.sleep(6)\n",
    "            NYT_api_requests = requests.get(get_URL)\n",
    "            NYT_api_convert = NYT_api_requests.json()\n",
    "            if x == 0:  ### Assiging the NYT_df as the foundation for building the DataFrame so the next pages can be .concat\n",
    "                if NYT_api_convert['status'] != \"OK\":  ### These checkes to see if the URL request was successful\n",
    "                    print(\"Error: \" + str(NYT_api_convert['status']) + \" ; Company: \" + y)\n",
    "                    break\n",
    "\n",
    "                if NYT_api_convert['status'] == \"OK\":\n",
    "                    NYT_df = pandas.DataFrame()\n",
    "                    NYT_df['request_result'] = NYT_api_convert['response']['docs']\n",
    "\n",
    "            else: ### Need to combine the pages as it iterates\n",
    "                if NYT_api_convert['status'] != \"OK\":\n",
    "                    print(\"Error: \" + str(NYT_api_convert['status']) + \" ; Company: \" + y)\n",
    "                    break\n",
    "\n",
    "                if NYT_api_convert['status'] == \"OK\":\n",
    "                    NYT_df_temp = pandas.DataFrame()\n",
    "                    NYT_df_temp['request_result'] = NYT_api_convert['response']['docs']  \n",
    "                    NYT_df = pandas.concat([NYT_df, NYT_df_temp], ignore_index=True)              \n",
    "\n",
    "        NYT_df['company'] = y\n",
    "        print('1')\n",
    "\n",
    "        for u, v in enumerate(NYT_df.request_result):   ### Extracting desired data from the nested dictionary JSON URL return\n",
    "            NYT_df.loc[u, 'headline'] = v['headline']['main']\n",
    "            NYT_df.loc[u, 'abstract'] = v['abstract']\n",
    "            NYT_df.loc[u, 'snippet'] = v['snippet']\n",
    "            NYT_df.loc[u, 'lead_paragraph'] = v['lead_paragraph']\n",
    "            NYT_df.loc[u, 'pub_date'] = v['pub_date']\n",
    "            NYT_df.loc[u, 'source'] = 'The New York Times'\n",
    "            NYT_df.loc[u, 'web_url'] = v['web_url']\n",
    "\n",
    "        ### This accounts for the case that Times API does not have any a results for the search but has saved a file in a previous run\n",
    "        ### If the existing file size is \"1 KB\", then it is most likely a empty dataframe with only two columns.\n",
    "        if 'pub_date' not in NYT_df.columns: \n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "            \n",
    "        ### Updates the current .csv file with the new dates, if it already exists.\n",
    "        if os.path.exists(filepath + y + '.csv') == True:\n",
    "            temp_df = pandas.read_csv(filepath + y + '.csv')\n",
    "            print('2')\n",
    "            #### This should cover the situation where pages != 1\n",
    "            master_df = pandas.concat([temp_df, NYT_df], ignore_index=True).drop_duplicates('headline').sort_values(by='pub_date', ascending=False)\n",
    "            print('3')\n",
    "            master_df.to_csv(filepath + y + '.csv')\n",
    "        print('7')\n",
    "        if os.path.exists(filepath + y + '.csv') == False:\n",
    "            print('6')\n",
    "            NYT_df.to_csv(filepath + y + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_search(stock_indices_df, pages=10) ### Always run first, 100 articles is typically 1-2 days for S&P\n",
    "\n",
    "article_search(stock_exchange_ticks_and_names_0_399)  ### \n",
    "article_search(stock_exchange_ticks_and_names_400_799)  ### \n",
    "article_search(stock_exchange_ticks_and_names_800_1199)  ### \n",
    "article_search(stock_exchange_ticks_and_names_1200_1599)  ### \n",
    "article_search(stock_exchange_ticks_and_names_1600_1999)  ### \n",
    "article_search(stock_exchange_ticks_and_names_2000_2399)  ### \n",
    "article_search(stock_exchange_ticks_and_names_2400_2799)  ### \n",
    "article_search(stock_exchange_ticks_and_names_2800_3199)  ### \n",
    "article_search(stock_exchange_ticks_and_names_3200_3599)  ### \n",
    "article_search(stock_exchange_ticks_and_names_3600_3999)  ### \n",
    "article_search(stock_exchange_ticks_and_names_4000_4399)  ### \n",
    "article_search(stock_exchange_ticks_and_names_4400_4799)  ### no stocks in df\n",
    "article_search(stock_exchange_ticks_and_names_4800_4880)  ### no stocks in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
